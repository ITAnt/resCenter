## 框架

### 常见框架

- DirectShow
- Media Foundation
- Helix
- FFmpeg
- Gstreamer
- VideoLAN
- Android Media
- AVFoundation

#### DirectShow

DirectShow的设计理是，开发者创建一个 Graph （也可称 Pipeline），将所有用到的组件（称作 Filter）加入 Graph 当中，当应用运行时，Graph 找到了注册的 Filter 并为之连接，Filter 之间形成一个 DAG（有向无环图）,开始播放环节。Filter 常用于对音视频流的某一步的处理，包括文件读取、渲染等。



#### Helix

##### Sure Stream(确定流)

Sure Stream(确定流）技术，即在一个文件或直播流内包含多种不同码率的音视频流，并在流媒体协议中探测播放器的带宽情况,选择不同的码率发送给播放器。

##### RSD

RSD(即 Reduce Startup Delay，降低启动延迟)是一项针对RTSP或RTMP协议流媒体传输时优化启动时间的技术，因为播放器在开始播放前需要得到足够的缓冲数据，且播放需要从关键帧开始。由于在服务器中直播流存在一定长度的缓冲，当客户端发起请求时，服务端首先将保证从关键帧开始发送，而非简单地发送所持有的时间截最早的包:其次，服务器会在发送初期按照3倍速率发送,以帮助播放器尽快填满缓冲区，开始播放。点播场景中的思路相似，服务端将在指定播放位置附近找到最近关键帧再行发送。

##### Fast Channel Switching

Fast Channel Switching，即快速频道切换，当播放器意图切换点播节目或直播频道时，其与服务端之间建立的连接并不销毁重建，而是复用已存在的Session，服务端从新节目的关键帧开始加速发送，可以避免重建播放会话带来的屏幕中断,并达到最快的节目切换。

##### Live Low Latency

Live Low Latency的技术原理并不复杂,Helix服务器支持多级级联，原本从每一级的服务器到终端的RealPlayer，各个环节上默认都有1~3s不等的缓冲。使用LLL技术时，服务器和播放器都会取消缓冲,并丢弃超时到达的音视频帧,保证时间线与原始时间线尽量接近，这项技术和 RSD 技术存在冲突。

##### Rate Control

Rate Control译为码率控制，这是一项专利技术,其目的是调整服务器向客户端发送数据的速度，以保证播放器播放的连续和保持合理大小的缓冲，服务器利用RTCP,通过评估客户端的请求到达时间和包丢失率来建立模型,推断网络状况。

##### Server Side Playlist 

Server Side Playlist 即服务端播放列表，这项技术主要用于广告插入，允许在服务端通过编辑播放列表的方式对点播和直播流插入广告,因为支持通过API动态加入和删除播放列表,广告插入的决定可以无限接近实时。

#### FFmpeg

- ffmpeg
- ffplay
- ffserver
- ffprobe

##### 语法

> ffmpeg [global_options] {[input_file_options] -i input_url} .. { [output_file_options]output_url} ..·

##### ffserver

ffserver 提供了简易的流媒体服务器功能，仅需将打算发布的视频文件准备好，在配置文件中进行几行设置，再行启动ffserver，就可以供人访问。通过ffimpeg 创建的音视频流可以被ffserver监听并发布出去，在 Linux系统中，默认config文件的位置是/etc/ffserver.conf。

#### Gstreamer

Gstreamer和 DirectShow很相似,是基于Pipeline的流行多媒体框架。
Gstreamer 中主要的慨念包括  Element、Pad、Bin、Bus 和 Pipeline 等。Element 是 GStreamer 中组件的名称，类似于 DirectShow 中的 Filter，可以被看作一个黑盒，主要包括 Source、Muxer、Demuxer、Convertor、Codec 、Sink 等种类。Pad 是 Element 之间或对外的 Interface(类似于 DirectShow 中的 Pin)，数据从一个 Src Pad 向下一个 Element 的 Sink Pad 流动，一个 Element 可以有多个Sink Pad 或 Src Pad，只有 Capability (可视为 Media Type，数据结构为 GstCap)相通的Pad 才可以相互连接，也即多个 Element 才可以相互连接。

Gstreamer 里的Bin是一个容器类型 Element ，其中允许含有一个或多个 Element，实际上它自己也可被看作一个 Element，这就允许一组连接好的Element 被简单地使用，最上游和最下游的 Sink Pad 和 Src Pad 可以被看作整个Bin 的Sink Pad 和 Src Pad 。Bus 是 Gstreamer 提供的机制，可将消息从 Element 的线程转发到应用的线程中，默认因为每个 Pipeline 都含有一个 Bus，所以应用程序并不需要自己创建它，只需要在 Bus上设置一个消息处理的回调函数，在程序运转时遇到新消息予以响应即可。借由 Element、Pad、Bin 和 Bus 的支持，不同的 Element连接在一起，就可以形成 Pipeline自动处理多媒体数据。

#### VideoLAN

简称 VLC，VLC 旗下有 x264

#### Android Media

- MediaPlayer
- MediaCodec
- MediaDRM
- MediaFormat
- MediaExtractor
- AudioManager
- MediaExtractor   用于提取音视频中一帧帧的数据
- MediaCodec        编解码

在 Android 的多媒体架构中，Java层的 API通过 Binder，于Native层作为代理的Media Player Service获得服务，Media Player Service负责为每一个上层应用创立 Session，调用更底层的组件，此时底层响应服务的可以是默认的 StagcFright Player，也可以是芯片或手机厂商自己的Player，例如 Mst Player、Mtk Player、Amlogic Player、Helix Player等。StageFright Player本身是使用Awesome Player 一个较薄的封装，早期的版本中预设的多媒体模块是 OpenCore，此处 Android定义了StageFright层以及OMX层进行了封装。


OMX又称 OpenMax，是由NVIDIA提出的多媒体应用的框架标准，定义了跨平台的应用程序接口API，帮助不同组件在不同操作系统和处理器硬件平台之间移植。OpenMax从下至上分为三层，包括OpenMax DL(开发层)、OpenMax IL(集成层)以及 OpenMax AL(应用层)，如图2-22所示。实际使用中，OpenMax IL多被用到,只要符合接口定义，不论是原生组件,还是软硬件厂商自行开发的组件,均可被集成使用。